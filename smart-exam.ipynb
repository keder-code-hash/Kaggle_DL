{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n!tar -xvzf data.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:02:52.650810Z","iopub.execute_input":"2022-02-18T17:02:52.651347Z","iopub.status.idle":"2022-02-18T17:03:04.227101Z","shell.execute_reply.started":"2022-02-18T17:02:52.651184Z","shell.execute_reply":"2022-02-18T17:03:04.226272Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 11.1M  100 11.1M    0     0  12.8M      0 --:--:-- --:--:-- --:--:-- 12.8M\nSNLI_Corpus/\nSNLI_Corpus/snli_1.0_dev.csv\nSNLI_Corpus/snli_1.0_train.csv\nSNLI_Corpus/snli_1.0_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:03:04.229267Z","iopub.execute_input":"2022-02-18T17:03:04.229525Z","iopub.status.idle":"2022-02-18T17:03:09.891628Z","shell.execute_reply.started":"2022-02-18T17:03:04.229487Z","shell.execute_reply":"2022-02-18T17:03:09.890939Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nbert_model.trainable = False\nmax_length = 256  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:03:09.892888Z","iopub.execute_input":"2022-02-18T17:03:09.893145Z","iopub.status.idle":"2022-02-18T17:21:43.092585Z","shell.execute_reply.started":"2022-02-18T17:03:09.893108Z","shell.execute_reply":"2022-02-18T17:21:43.091911Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"821e4be307784843ae0729a6ac6ac219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"777de16b3ecf4015af04fc80041718f6"}},"metadata":{}},{"name":"stderr","text":"2022-02-18 17:21:36.199414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:36.200459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:36.201121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:36.202157: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-02-18 17:21:36.203256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:36.203876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:36.204507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:40.682489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:40.683298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:40.684120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-18 17:21:40.684775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# There are more than 550k samples in total; we will use 100k for this example.\ntrain_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=150000) # 20k\nvalid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")   # 10k\ntest_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.094692Z","iopub.execute_input":"2022-02-18T17:21:43.094951Z","iopub.status.idle":"2022-02-18T17:21:43.333853Z","shell.execute_reply.started":"2022-02-18T17:21:43.094916Z","shell.execute_reply":"2022-02-18T17:21:43.333141Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total train samples : 150000\nTotal validation samples: 10000\nTotal test samples: 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\nprint(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\nprint(f\"Similarity: {train_df.loc[1, 'similarity']}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.334959Z","iopub.execute_input":"2022-02-18T17:21:43.335763Z","iopub.status.idle":"2022-02-18T17:21:43.349513Z","shell.execute_reply.started":"2022-02-18T17:21:43.335723Z","shell.execute_reply":"2022-02-18T17:21:43.348739Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Sentence1: A person on a horse jumps over a broken down airplane.\nSentence2: A person is at a diner, ordering an omelette.\nSimilarity: contradiction\n","output_type":"stream"}]},{"cell_type":"code","source":"# We have some NaN entries in our train data, we will simply drop them.\nprint(\"Number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.351073Z","iopub.execute_input":"2022-02-18T17:21:43.351446Z","iopub.status.idle":"2022-02-18T17:21:43.451806Z","shell.execute_reply.started":"2022-02-18T17:21:43.351411Z","shell.execute_reply":"2022-02-18T17:21:43.451089Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of missing values\nsimilarity    0\nsentence1     0\nsentence2     3\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Train Target Distribution\")\nprint(train_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.453154Z","iopub.execute_input":"2022-02-18T17:21:43.453552Z","iopub.status.idle":"2022-02-18T17:21:43.475198Z","shell.execute_reply.started":"2022-02-18T17:21:43.453516Z","shell.execute_reply":"2022-02-18T17:21:43.474415Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Train Target Distribution\nentailment       50053\ncontradiction    49951\nneutral          49816\n-                  177\nName: similarity, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Validation Target Distribution\")\nprint(valid_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.476557Z","iopub.execute_input":"2022-02-18T17:21:43.476805Z","iopub.status.idle":"2022-02-18T17:21:43.483597Z","shell.execute_reply.started":"2022-02-18T17:21:43.476771Z","shell.execute_reply":"2022-02-18T17:21:43.482938Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Validation Target Distribution\nentailment       3329\ncontradiction    3278\nneutral          3235\n-                 158\nName: similarity, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = (\n    train_df[train_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)\nvalid_df = (\n    valid_df[valid_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.484774Z","iopub.execute_input":"2022-02-18T17:21:43.485494Z","iopub.status.idle":"2022-02-18T17:21:43.541241Z","shell.execute_reply.started":"2022-02-18T17:21:43.485457Z","shell.execute_reply":"2022-02-18T17:21:43.540443Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df[\"label\"] = train_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.544823Z","iopub.execute_input":"2022-02-18T17:21:43.545271Z","iopub.status.idle":"2022-02-18T17:21:43.661434Z","shell.execute_reply.started":"2022-02-18T17:21:43.545232Z","shell.execute_reply":"2022-02-18T17:21:43.660699Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        \n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        \n        sequence_output = bert_output.last_hidden_state\n#         pooled_output = bert_output.pooler_output\n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:21:43.662926Z","iopub.execute_input":"2022-02-18T17:21:43.663229Z","iopub.status.idle":"2022-02-18T17:21:43.675154Z","shell.execute_reply.started":"2022-02-18T17:21:43.663181Z","shell.execute_reply":"2022-02-18T17:21:43.674394Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope(): \n    input_layer = tf.keras.layers.Input(shape=(256, 768), name=None)\n#     input_layer=tf.reshape(input_layer, (128, 768), name=None) \n    \n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(input_layer) \n    \n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.2)(concat)    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=input_layer, outputs=output\n    )\n    \n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T17:28:12.795384Z","iopub.execute_input":"2022-02-18T17:28:12.795581Z","iopub.status.idle":"2022-02-18T17:28:13.223846Z","shell.execute_reply.started":"2022-02-18T17:28:12.795556Z","shell.execute_reply":"2022-02-18T17:28:13.223139Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 256, 768)]   0                                            \n__________________________________________________________________________________________________\nbidirectional_4 (Bidirectional) (None, 256, 256)     918528      input_5[0][0]                    \n__________________________________________________________________________________________________\nglobal_average_pooling1d_4 (Glo (None, 256)          0           bidirectional_4[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_4 (GlobalM (None, 256)          0           bidirectional_4[0][0]            \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling1d_4[0][0] \n                                                                 global_max_pooling1d_4[0][0]     \n__________________________________________________________________________________________________\ndropout_41 (Dropout)            (None, 512)          0           concatenate_4[0][0]              \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 3)            1539        dropout_41[0][0]                 \n==================================================================================================\nTotal params: 920,067\nTrainable params: 920,067\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:58:49.408986Z","iopub.execute_input":"2022-02-18T01:58:49.409276Z","iopub.status.idle":"2022-02-18T01:58:49.429058Z","shell.execute_reply.started":"2022-02-18T01:58:49.409236Z","shell.execute_reply":"2022-02-18T01:58:49.428187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:58:49.432497Z","iopub.execute_input":"2022-02-18T01:58:49.433227Z","iopub.status.idle":"2022-02-18T01:58:56.762002Z","shell.execute_reply.started":"2022-02-18T01:58:49.433186Z","shell.execute_reply":"2022-02-18T01:58:56.761159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check size of returned array \n\nprint(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:58:56.763394Z","iopub.execute_input":"2022-02-18T01:58:56.763758Z","iopub.status.idle":"2022-02-18T01:58:57.131448Z","shell.execute_reply.started":"2022-02-18T01:58:56.763719Z","shell.execute_reply":"2022-02-18T01:58:57.128327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T01:58:57.132578Z","iopub.execute_input":"2022-02-18T01:58:57.132943Z","iopub.status.idle":"2022-02-18T02:57:13.490016Z","shell.execute_reply.started":"2022-02-18T01:58:57.132909Z","shell.execute_reply":"2022-02-18T02:57:13.489282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"testmodel0.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:57:13.49199Z","iopub.execute_input":"2022-02-18T02:57:13.492261Z","iopub.status.idle":"2022-02-18T02:57:13.540776Z","shell.execute_reply.started":"2022-02-18T02:57:13.492222Z","shell.execute_reply":"2022-02-18T02:57:13.540077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.trainable = True\n# Recompile the model to make the change effective.\ntrain_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:57:13.542171Z","iopub.execute_input":"2022-02-18T02:57:13.542426Z","iopub.status.idle":"2022-02-18T02:57:19.144783Z","shell.execute_reply.started":"2022-02-18T02:57:13.54239Z","shell.execute_reply":"2022-02-18T02:57:19.142791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=1,\n    use_multiprocessing=True,\n    workers=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:58:21.295801Z","iopub.execute_input":"2022-02-18T02:58:21.296344Z","iopub.status.idle":"2022-02-18T03:28:43.606287Z","shell.execute_reply.started":"2022-02-18T02:58:21.296302Z","shell.execute_reply":"2022-02-18T03:28:43.605339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"fine_tuned_testmodel1.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:35:05.573227Z","iopub.execute_input":"2022-02-18T03:35:05.573876Z","iopub.status.idle":"2022-02-18T03:35:05.620677Z","shell.execute_reply.started":"2022-02-18T03:35:05.573834Z","shell.execute_reply":"2022-02-18T03:35:05.61998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Code","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\ncus_mod=load_model('./fine_tuned_testmodel1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:35:33.360712Z","iopub.execute_input":"2022-02-18T03:35:33.360985Z","iopub.status.idle":"2022-02-18T03:35:33.83705Z","shell.execute_reply.started":"2022-02-18T03:35:33.360948Z","shell.execute_reply":"2022-02-18T03:35:33.836318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence1='Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.'\nsentence2='Deep learning is a kind of machine learning where a computer analyzes algorithms and their results to \"learn\" ways of improving processes and creating new ones.'\ns3 = 'Deep learning can be considered as a subset of machine learning. It is a field that is based on learning and improving on its own by examining computer algorithms.'\ns4 = 'Deep learning can  be considered as a subset of machine learning. It a field that is based on learning and improving on its own by examining computer algorithms.'\nts1=\"Kinetic energy, form of energy that an object or a particle has by reason of its motion.\"\nts2=\"Kinetic energy is not the energy an object has because of its motion.\"\nts3=\"Energy is the quantitative property that must be transferred to a body or physical system to perform work on the body, or to heat it.\"","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:35:37.144187Z","iopub.execute_input":"2022-02-18T03:35:37.144715Z","iopub.status.idle":"2022-02-18T03:35:37.149466Z","shell.execute_reply.started":"2022-02-18T03:35:37.144675Z","shell.execute_reply":"2022-02-18T03:35:37.148637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_pairs = np.array([[str(s3), str(ts3)]])\ntest_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\npro=cus_mod.predict(test_data)\nprint(pro)\n# [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-18T03:36:49.370611Z","iopub.execute_input":"2022-02-18T03:36:49.37087Z","iopub.status.idle":"2022-02-18T03:36:55.248971Z","shell.execute_reply.started":"2022-02-18T03:36:49.37084Z","shell.execute_reply":"2022-02-18T03:36:55.247517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:58:09.126031Z","iopub.status.idle":"2022-02-18T02:58:09.130591Z","shell.execute_reply.started":"2022-02-18T02:58:09.130277Z","shell.execute_reply":"2022-02-18T02:58:09.130311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the model trained for 200 total epochs loss curves\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable","metadata":{"execution":{"iopub.status.busy":"2022-02-18T02:58:09.132018Z","iopub.status.idle":"2022-02-18T02:58:09.132745Z","shell.execute_reply.started":"2022-02-18T02:58:09.132486Z","shell.execute_reply":"2022-02-18T02:58:09.132516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}