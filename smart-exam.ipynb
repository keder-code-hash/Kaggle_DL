{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n!tar -xvzf data.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:04.790920Z","iopub.execute_input":"2022-02-17T19:14:04.791233Z","iopub.status.idle":"2022-02-17T19:14:15.774761Z","shell.execute_reply.started":"2022-02-17T19:14:04.791151Z","shell.execute_reply":"2022-02-17T19:14:15.773987Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 11.1M  100 11.1M    0     0  16.6M      0 --:--:-- --:--:-- --:--:-- 16.5M\nSNLI_Corpus/\nSNLI_Corpus/snli_1.0_dev.csv\nSNLI_Corpus/snli_1.0_train.csv\nSNLI_Corpus/snli_1.0_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:19.152565Z","iopub.execute_input":"2022-02-17T19:14:19.152845Z","iopub.status.idle":"2022-02-17T19:14:24.871462Z","shell.execute_reply.started":"2022-02-17T19:14:19.152799Z","shell.execute_reply":"2022-02-17T19:14:24.870736Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nbert_model.trainable = False\nmax_length = 256  # Maximum length of input sentence to the model.\nbatch_size = 32\nepochs = 2\n\n# Labels in our dataset.\nlabels = [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:24.873014Z","iopub.execute_input":"2022-02-17T19:14:24.873245Z","iopub.status.idle":"2022-02-17T19:14:46.151330Z","shell.execute_reply.started":"2022-02-17T19:14:24.873212Z","shell.execute_reply":"2022-02-17T19:14:46.150589Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd45dd7858343f5868383d97345042a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77021e3267d54dcca86edd0ab4b2e5e4"}},"metadata":{}},{"name":"stderr","text":"2022-02-17 19:14:38.965846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:38.966872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:38.967513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:38.968342: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-02-17 19:14:38.970153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:38.970886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:38.971542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:43.776254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:43.777311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:43.778327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-02-17 19:14:43.779299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# There are more than 550k samples in total; we will use 100k for this example.\ntrain_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=150000) # 20k\nvalid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")   # 10k\ntest_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n\n# Shape of the data\nprint(f\"Total train samples : {train_df.shape[0]}\")\nprint(f\"Total validation samples: {valid_df.shape[0]}\")\nprint(f\"Total test samples: {valid_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.152764Z","iopub.execute_input":"2022-02-17T19:14:46.153043Z","iopub.status.idle":"2022-02-17T19:14:46.407442Z","shell.execute_reply.started":"2022-02-17T19:14:46.153007Z","shell.execute_reply":"2022-02-17T19:14:46.406549Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total train samples : 150000\nTotal validation samples: 10000\nTotal test samples: 10000\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\nprint(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\nprint(f\"Similarity: {train_df.loc[1, 'similarity']}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.409362Z","iopub.execute_input":"2022-02-17T19:14:46.409682Z","iopub.status.idle":"2022-02-17T19:14:46.422958Z","shell.execute_reply.started":"2022-02-17T19:14:46.409645Z","shell.execute_reply":"2022-02-17T19:14:46.422085Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Sentence1: A person on a horse jumps over a broken down airplane.\nSentence2: A person is at a diner, ordering an omelette.\nSimilarity: contradiction\n","output_type":"stream"}]},{"cell_type":"code","source":"# We have some NaN entries in our train data, we will simply drop them.\nprint(\"Number of missing values\")\nprint(train_df.isnull().sum())\ntrain_df.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.424542Z","iopub.execute_input":"2022-02-17T19:14:46.425065Z","iopub.status.idle":"2022-02-17T19:14:46.526445Z","shell.execute_reply.started":"2022-02-17T19:14:46.425025Z","shell.execute_reply":"2022-02-17T19:14:46.525738Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of missing values\nsimilarity    0\nsentence1     0\nsentence2     3\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Train Target Distribution\")\nprint(train_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.527778Z","iopub.execute_input":"2022-02-17T19:14:46.528192Z","iopub.status.idle":"2022-02-17T19:14:46.550145Z","shell.execute_reply.started":"2022-02-17T19:14:46.528155Z","shell.execute_reply":"2022-02-17T19:14:46.549424Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Train Target Distribution\nentailment       50053\ncontradiction    49951\nneutral          49816\n-                  177\nName: similarity, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Validation Target Distribution\")\nprint(valid_df.similarity.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.551375Z","iopub.execute_input":"2022-02-17T19:14:46.551783Z","iopub.status.idle":"2022-02-17T19:14:46.559104Z","shell.execute_reply.started":"2022-02-17T19:14:46.551748Z","shell.execute_reply":"2022-02-17T19:14:46.558284Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Validation Target Distribution\nentailment       3329\ncontradiction    3278\nneutral          3235\n-                 158\nName: similarity, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = (\n    train_df[train_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)\nvalid_df = (\n    valid_df[valid_df.similarity != \"-\"]\n    .sample(frac=1.0, random_state=42)\n    .reset_index(drop=True)\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.560531Z","iopub.execute_input":"2022-02-17T19:14:46.560793Z","iopub.status.idle":"2022-02-17T19:14:46.616707Z","shell.execute_reply.started":"2022-02-17T19:14:46.560757Z","shell.execute_reply":"2022-02-17T19:14:46.615985Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df[\"label\"] = train_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n\nvalid_df[\"label\"] = valid_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n\ntest_df[\"label\"] = test_df[\"similarity\"].apply(\n    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n)\ny_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.618929Z","iopub.execute_input":"2022-02-17T19:14:46.619266Z","iopub.status.idle":"2022-02-17T19:14:46.724588Z","shell.execute_reply.started":"2022-02-17T19:14:46.619227Z","shell.execute_reply":"2022-02-17T19:14:46.723939Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n        \n        # Load our BERT Tokenizer to encode the text.\n        # We will use base-base-uncased pretrained model.\n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=max_length,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        \n        sequence_output = bert_output.last_hidden_state\n#         pooled_output = bert_output.pooler_output\n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self):\n        # Shuffle indexes after each epoch if shuffle is set to True.\n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:14:46.725774Z","iopub.execute_input":"2022-02-17T19:14:46.726057Z","iopub.status.idle":"2022-02-17T19:14:46.737671Z","shell.execute_reply.started":"2022-02-17T19:14:46.726027Z","shell.execute_reply":"2022-02-17T19:14:46.736673Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\nwith strategy.scope(): \n    input_layer = tf.keras.layers.Input(shape=(256, 768), name=None)\n#     input_layer=tf.reshape(input_layer, (128, 768), name=None) \n    \n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(input_layer) \n    \n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n    model = tf.keras.models.Model(\n        inputs=input_layer, outputs=output\n    )\n    \n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:16:28.391563Z","iopub.execute_input":"2022-02-17T19:16:28.391847Z","iopub.status.idle":"2022-02-17T19:16:28.866242Z","shell.execute_reply.started":"2022-02-17T19:16:28.391800Z","shell.execute_reply":"2022-02-17T19:16:28.865520Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 256, 768)]   0                                            \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 256, 128)     426496      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 427,267\nTrainable params: 427,267\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:16:37.204090Z","iopub.execute_input":"2022-02-17T19:16:37.204358Z","iopub.status.idle":"2022-02-17T19:16:37.226327Z","shell.execute_reply.started":"2022-02-17T19:16:37.204329Z","shell.execute_reply":"2022-02-17T19:16:37.225402Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:16:48.786733Z","iopub.execute_input":"2022-02-17T19:16:48.787259Z","iopub.status.idle":"2022-02-17T19:16:54.323366Z","shell.execute_reply.started":"2022-02-17T19:16:48.787221Z","shell.execute_reply":"2022-02-17T19:16:54.322644Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# check size of returned array \n\nprint(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:16:04.207471Z","iopub.execute_input":"2022-02-17T19:16:04.207727Z","iopub.status.idle":"2022-02-17T19:16:04.561052Z","shell.execute_reply.started":"2022-02-17T19:16:04.207698Z","shell.execute_reply":"2022-02-17T19:16:04.560248Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(<tf.Tensor: shape=(32, 256, 768), dtype=float32, numpy=\narray([[[-4.81573522e-01,  1.67410627e-01, -8.92172039e-01, ...,\n         -5.98917186e-01,  6.42276049e-01,  1.80563152e-01],\n        [-1.28185272e-01,  1.86316729e-01, -7.92089224e-01, ...,\n         -3.82064670e-01,  2.30369419e-01,  2.90217072e-01],\n        [ 3.53533000e-01, -3.66870135e-01,  6.07352495e-01, ...,\n         -9.52024221e-01,  6.00744426e-01, -5.10293722e-01],\n        ...,\n        [-1.05336756e-02, -2.98894733e-01,  1.54385999e-01, ...,\n          5.01793742e-01,  1.12700343e-01, -4.95263994e-01],\n        [-2.03743741e-01, -5.56390584e-01,  5.35488129e-04, ...,\n          4.30884212e-01,  1.82686254e-01, -4.66849685e-01],\n        [ 2.27108970e-01, -8.24976265e-02,  2.73351133e-01, ...,\n         -1.43957272e-01, -2.12298185e-01, -1.38788402e-01]],\n\n       [[-5.07904232e-01,  4.90119904e-01, -5.86317778e-01, ...,\n         -4.91990149e-01,  3.04003388e-01,  1.07394144e-01],\n        [-2.06783429e-01,  2.50541270e-01, -4.72156078e-01, ...,\n         -4.44341570e-01, -5.41445054e-02, -1.71844423e-01],\n        [ 1.96023375e-01, -8.82432386e-02,  1.26354381e-01, ...,\n         -4.11051899e-01,  1.60535835e-02, -8.30307424e-01],\n        ...,\n        [ 3.39060351e-02, -1.36993647e-01,  1.00304559e-01, ...,\n          2.43491866e-02,  4.39424999e-02, -2.96001043e-02],\n        [-7.85887390e-02, -1.46234855e-01,  1.38515458e-02, ...,\n          1.71718597e-01,  1.22806720e-01, -2.66354680e-01],\n        [ 3.94695401e-01,  3.02632581e-02,  2.97481686e-01, ...,\n         -6.05492294e-03,  8.85093287e-02, -2.45246217e-01]],\n\n       [[-7.58087993e-01,  4.65444088e-01, -7.67330468e-01, ...,\n         -5.65325439e-01,  7.55696833e-01,  3.87741104e-02],\n        [ 1.41353697e-01,  1.41350664e-02, -6.55390501e-01, ...,\n         -1.98072791e-01,  9.30457234e-01, -1.62461400e-01],\n        [ 2.50714898e-01,  6.00298166e-01, -3.73409927e-01, ...,\n         -5.12234211e-01,  5.29924631e-01, -9.35449064e-01],\n        ...,\n        [ 1.95925444e-01,  1.24024972e-02,  2.54808329e-02, ...,\n          2.04485089e-01,  3.55578959e-01, -2.32798651e-01],\n        [ 6.17967658e-02,  4.39166604e-03, -1.00382566e-02, ...,\n          3.15570980e-01,  4.11542624e-01, -3.85329962e-01],\n        [ 2.35190205e-02,  6.13757968e-03, -1.51086658e-01, ...,\n          1.49654582e-01,  1.29144579e-01, -1.27752990e-01]],\n\n       ...,\n\n       [[-8.22837234e-01,  4.64313626e-01, -5.09569407e-01, ...,\n         -6.55698419e-01,  5.19551933e-01, -9.24585462e-02],\n        [-1.12298697e-01, -1.57541901e-01, -5.79964340e-01, ...,\n         -3.68126661e-01,  5.11135817e-01, -1.76522285e-01],\n        [-4.12213594e-01, -1.23642627e-02,  3.33545566e-01, ...,\n         -8.09485555e-01,  4.86833632e-01, -6.48277760e-01],\n        ...,\n        [ 9.99382883e-02, -1.21111654e-01,  9.54279602e-02, ...,\n          1.82533618e-02,  2.57573277e-01,  1.23606384e-01],\n        [ 9.69803482e-02, -1.43528339e-02,  2.74083674e-01, ...,\n         -1.11553639e-01,  2.73749888e-01,  7.69729838e-02],\n        [-7.67309889e-02,  2.65664995e-01,  3.41626674e-01, ...,\n          2.07908601e-01, -2.29036957e-02, -2.28483021e-01]],\n\n       [[-1.01566899e+00,  4.38764483e-01, -2.51798868e-01, ...,\n         -1.02228725e+00,  4.86373961e-01,  4.28123586e-02],\n        [-1.77879818e-03, -2.10497692e-01, -6.04683280e-01, ...,\n         -4.64657962e-01,  7.49116302e-01, -2.85585672e-01],\n        [-3.46725941e-01, -9.04593766e-02, -2.37450987e-01, ...,\n         -7.63330042e-01, -3.31624225e-02, -1.06339657e+00],\n        ...,\n        [ 1.68928072e-01, -1.12764858e-01,  2.46896327e-01, ...,\n          1.74691156e-03,  2.00478956e-01,  6.48997724e-02],\n        [ 2.68634185e-02, -1.74558803e-01,  3.24960619e-01, ...,\n         -1.49038911e-01,  2.51224995e-01, -1.69036612e-01],\n        [-1.60666645e-01, -2.80968189e-01,  4.63093638e-01, ...,\n         -5.53988293e-02,  2.25054994e-01, -2.58737177e-01]],\n\n       [[-8.74657154e-01,  4.15915817e-01, -5.22675455e-01, ...,\n         -7.88512170e-01,  3.82251501e-01,  5.42188361e-02],\n        [-3.20177913e-01,  3.16479802e-01, -6.01734042e-01, ...,\n         -5.99053383e-01,  3.11252207e-01, -2.48698249e-01],\n        [ 9.80942845e-02, -8.06004584e-01,  1.21651575e-01, ...,\n         -5.53194046e-01,  4.20955494e-02, -1.94390297e-01],\n        ...,\n        [-2.64953971e-01,  6.57709539e-02,  2.13264167e-01, ...,\n         -3.35010707e-01,  1.52149707e-01,  5.25755882e-02],\n        [-9.18387622e-02,  4.58929464e-02,  1.68894917e-01, ...,\n         -1.82242796e-01,  9.45616066e-02, -6.00019619e-02],\n        [-7.92751461e-02, -2.97577173e-01,  1.81862891e-01, ...,\n         -1.31171882e-01,  3.39018479e-02, -2.22199529e-01]]],\n      dtype=float32)>, array([[0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0],\n       [0, 1, 0]], dtype=int32))\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T19:17:21.439318Z","iopub.execute_input":"2022-02-17T19:17:21.439584Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n2022-02-17 19:17:21.811816: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\nop: \"FlatMapDataset\"\ninput: \"TensorDataset/_1\"\nattr {\n  key: \"Targuments\"\n  value {\n    list {\n    }\n  }\n}\nattr {\n  key: \"f\"\n  value {\n    func {\n      name: \"__inference_Dataset_flat_map_flat_map_fn_11218\"\n    }\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n    }\n  }\n}\nattr {\n  key: \"output_types\"\n  value {\n    list {\n      type: DT_FLOAT\n      type: DT_INT32\n    }\n  }\n}\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n2022-02-17 19:17:21.836764: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"2022-02-17 19:17:27.363741: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":" 548/4681 [==>...........................] - ETA: 23:57 - loss: 0.8515 - acc: 0.6073","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"1957/4681 [===========>..................] - ETA: 15:50 - loss: 0.6960 - acc: 0.7011","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"testmodel0.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:44:21.695604Z","iopub.execute_input":"2022-02-17T18:44:21.695847Z","iopub.status.idle":"2022-02-17T18:44:21.744339Z","shell.execute_reply.started":"2022-02-17T18:44:21.695814Z","shell.execute_reply":"2022-02-17T18:44:21.743609Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"bert_model.trainable = True\n# Recompile the model to make the change effective.\ntrain_data = BertSemanticDataGenerator(\n    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_train,\n    batch_size=batch_size,\n    shuffle=True,\n)\nvalid_data = BertSemanticDataGenerator(\n    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n    y_val,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:44:21.745794Z","iopub.execute_input":"2022-02-17T18:44:21.746050Z","iopub.status.idle":"2022-02-17T18:44:35.616808Z","shell.execute_reply.started":"2022-02-17T18:44:21.746015Z","shell.execute_reply":"2022-02-17T18:44:35.615846Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 128, 768)]   0                                            \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 128, 128)     426496      input_1[0][0]                    \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 128)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndropout_37 (Dropout)            (None, 256)          0           concatenate[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         dropout_37[0][0]                 \n==================================================================================================\nTotal params: 427,267\nTrainable params: 427,267\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=valid_data,\n    epochs=epochs,\n    use_multiprocessing=True,\n    workers=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:44:35.618028Z","iopub.execute_input":"2022-02-17T18:44:35.618329Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n2022-02-17 18:44:35.772826: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\nop: \"FlatMapDataset\"\ninput: \"TensorDataset/_1\"\nattr {\n  key: \"Targuments\"\n  value {\n    list {\n    }\n  }\n}\nattr {\n  key: \"f\"\n  value {\n    func {\n      name: \"__inference_Dataset_flat_map_flat_map_fn_14481365\"\n    }\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n    }\n  }\n}\nattr {\n  key: \"output_types\"\n  value {\n    list {\n      type: DT_FLOAT\n      type: DT_INT32\n    }\n  }\n}\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n 760/4681 [===>..........................] - ETA: 22:57 - loss: 0.4972 - accuracy: 0.8036","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"1078/4681 [=====>........................] - ETA: 21:04 - loss: 0.4920 - accuracy: 0.8056","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"2317/4681 [=============>................] - ETA: 13:47 - loss: 0.4872 - accuracy: 0.8082","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"2541/4681 [===============>..............] - ETA: 12:29 - loss: 0.4863 - accuracy: 0.8081","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"3030/4681 [==================>...........] - ETA: 9:38 - loss: 0.4845 - accuracy: 0.8088","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"3457/4681 [=====================>........] - ETA: 7:08 - loss: 0.4847 - accuracy: 0.8090","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"4681/4681 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8102","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n2022-02-17 19:12:00.650779: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:695] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\nop: \"FlatMapDataset\"\ninput: \"TensorDataset/_1\"\nattr {\n  key: \"Targuments\"\n  value {\n    list {\n    }\n  }\n}\nattr {\n  key: \"f\"\n  value {\n    func {\n      name: \"__inference_Dataset_flat_map_flat_map_fn_21275123\"\n    }\n  }\n}\nattr {\n  key: \"output_shapes\"\n  value {\n    list {\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n      shape {\n        dim {\n          size: -1\n        }\n        dim {\n          size: -1\n        }\n      }\n    }\n  }\n}\nattr {\n  key: \"output_types\"\n  value {\n    list {\n      type: DT_FLOAT\n      type: DT_INT32\n    }\n  }\n}\n. Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"fine_tuned_testmodel1.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Code","metadata":{}},{"cell_type":"code","source":"from keras.models import load_model\ncus_mod=load_model('./testmodel1.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence1='Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.'\nsentence2='Deep learning is a kind of machine learning where a computer analyzes algorithms and their results to \"learn\" ways of improving processes and creating new ones.'\ns3 = 'Deep learning can be considered as a subset of machine learning. It is a field that is based on learning and improving on its own by examining computer algorithms.'\ns4 = 'Deep learning can  be considered as a subset of machine learning. It a field that is based on learning and improving on its own by examining computer algorithms.'\nts1=\"Kinetic energy, form of energy that an object or a particle has by reason of its motion.\"\nts2=\"Kinetic energy is not the energy an object has because of its motion.\"\nts3=\"Energy is the quantitative property that must be transferred to a body or physical system to perform work on the body, or to heat it.\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_pairs = np.array([[str(ts1), str(ts3)]])\ntest_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\npro=cus_mod.predict(test_data)\nprint(pro)\n# [\"contradiction\", \"entailment\", \"neutral\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the model trained for 200 total epochs loss curves\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}