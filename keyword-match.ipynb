{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers ","metadata":{"id":"D6e6PitCbfZv","execution":{"iopub.status.busy":"2022-01-11T18:20:28.519833Z","iopub.execute_input":"2022-01-11T18:20:28.520098Z","iopub.status.idle":"2022-01-11T18:20:36.644174Z","shell.execute_reply.started":"2022-01-11T18:20:28.520069Z","shell.execute_reply":"2022-01-11T18:20:36.643380Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.12.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.7.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.23.2)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.9.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.1)\nRequirement already satisfied: tokenizers>=0.10.3 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.10.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (1.19.5)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (0.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers) (4.62.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.10.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (1.1.0)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers) (8.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keybert","metadata":{"id":"tvi8uQ8JxZMa","execution":{"iopub.status.busy":"2022-01-11T18:20:36.647321Z","iopub.execute_input":"2022-01-11T18:20:36.647637Z","iopub.status.idle":"2022-01-11T18:20:44.418313Z","shell.execute_reply.started":"2022-01-11T18:20:36.647592Z","shell.execute_reply":"2022-01-11T18:20:44.417505Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: keybert in /opt/conda/lib/python3.7/site-packages (0.5.0)\nRequirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.7/site-packages (from keybert) (0.23.2)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.7/site-packages (from keybert) (1.19.5)\nRequirement already satisfied: sentence-transformers>=0.3.8 in /opt/conda/lib/python3.7/site-packages (from keybert) (2.1.0)\nRequirement already satisfied: rich>=10.4.0 in /opt/conda/lib/python3.7/site-packages (from keybert) (10.16.1)\nRequirement already satisfied: typing-extensions<5.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (3.10.0.2)\nRequirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (0.4.4)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (2.10.0)\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (0.9.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (3.0.0)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (4.62.3)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.10.1)\nRequirement already satisfied: tokenizers>=0.10.3 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.10.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (1.9.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (4.12.5)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.10.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.11.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.0.46)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.26.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (6.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.16.0)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (8.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.0.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.26.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd   \nimport sklearn \nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import manhattan_distances\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import metrics  \nfrom scipy.spatial import distance\nfrom sentence_transformers import SentenceTransformer \nfrom keybert import KeyBERT\nkw_model = KeyBERT()\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{"id":"g1d74Bq1ntnI","execution":{"iopub.status.busy":"2022-01-11T18:20:44.420817Z","iopub.execute_input":"2022-01-11T18:20:44.421107Z","iopub.status.idle":"2022-01-11T18:20:48.256308Z","shell.execute_reply.started":"2022-01-11T18:20:44.421067Z","shell.execute_reply":"2022-01-11T18:20:48.255574Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"dataframe=pd.read_csv(\"../input/compiler-design-dataset/Compiler Design Dataset.csv\")\ndataframe.shape","metadata":{"id":"TozJrMCTark0","execution":{"iopub.status.busy":"2022-01-11T18:20:48.258194Z","iopub.execute_input":"2022-01-11T18:20:48.258460Z","iopub.status.idle":"2022-01-11T18:20:48.273541Z","shell.execute_reply.started":"2022-01-11T18:20:48.258427Z","shell.execute_reply":"2022-01-11T18:20:48.272918Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"(10, 5)"},"metadata":{}}]},{"cell_type":"code","source":"# standard_answer=dataframe['Questioner Answer'][6]\n# student_answer=dataframe['''Student's Answer'''][6] ","metadata":{"id":"3LTBSGOUkynH","execution":{"iopub.status.busy":"2022-01-11T18:20:48.274669Z","iopub.execute_input":"2022-01-11T18:20:48.274908Z","iopub.status.idle":"2022-01-11T18:20:48.279002Z","shell.execute_reply.started":"2022-01-11T18:20:48.274874Z","shell.execute_reply":"2022-01-11T18:20:48.278224Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"class SimilarityMetric:\n    def __init__(self,student_answer,standard_answer,st_ans,sn_ans,model=model) -> None:\n        self.student_answer=student_answer\n        self.standard_answer=standard_answer \n        self.st_ans=st_ans\n        self.sn_ans=sn_ans \n        self.embded_student_answer = model.encode(student_answer)\n        self.embded_standard_answer= model.encode(standard_answer)\n\n    def euclidian_dist(self): \n        dist=euclidean_distances(self.embded_standard_answer,self.embded_student_answer) \n        result=0.0\n        for d in dist: \n            result=min(d)\n        return result/dist.shape[0]\n    \n    def manhatten_dist(self):\n        dist=manhattan_distances( self.embded_standard_answer,self.embded_student_answer) \n        result=0.0\n        for d in dist: \n            result=min(d)\n        return result/dist.shape[0]\n#         return manhattan_distances( self.embded_standard_answer,self.embded_student_answer) \n\n    def cosine_similarity(self): \n        distances = cosine_similarity( self.embded_standard_answer,self.embded_student_answer)\n        result=0.0\n        for d in distances: \n            result=max(d)\n        return result/distances.shape[0] \n    \n    def Jaccard_Similarity(self): \n\n        words_doc1 = set(self.st_ans.lower().split()) \n        words_doc2 = set(self.sn_ans.lower().split())\n\n        intersection = words_doc1.intersection(words_doc2)\n\n        union = words_doc1.union(words_doc2)\n\n        return float(len(intersection)) / len(union)\n\n","metadata":{"id":"SH5IWT2Lb7e5","execution":{"iopub.status.busy":"2022-01-11T18:20:48.280297Z","iopub.execute_input":"2022-01-11T18:20:48.280871Z","iopub.status.idle":"2022-01-11T18:20:48.292685Z","shell.execute_reply.started":"2022-01-11T18:20:48.280836Z","shell.execute_reply":"2022-01-11T18:20:48.291900Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"cosine_similar=[]\neuclid_dist=[]\nmanhatten_d=[]\njaccard_Similarity=[]\nstnd_ans=[]\nstud_ans=[]\n_id=[]\nactual_sentiments=[]\nfor i in range(dataframe.shape[0]):\n    standard_answer=dataframe['Questioner Answer'][i]\n    student_answer=dataframe['''Student's Answer'''][i] \n    keywords_stnd = kw_model.extract_keywords([standard_answer], keyphrase_ngram_range=(1, 3), stop_words='english', use_mmr=True, diversity=0.3) \n    keywords_stud = kw_model.extract_keywords([student_answer], keyphrase_ngram_range=(1, 3), stop_words='english', use_mmr=True, diversity=0.3)\n    candidates_standard=[]\n    candidates_student=[]\n\n    for key,match in keywords_stnd[0]: \n      candidates_standard.append(key)\n    for key,match in keywords_stud[0]:\n      candidates_student.append(key) \n\n    sim_obj=SimilarityMetric(candidates_standard,candidates_student,standard_answer,student_answer)\n    \n    cosine_similar.append(sim_obj.cosine_similarity())\n    euclid_dist.append(sim_obj.euclidian_dist())\n    manhatten_d.append(sim_obj.manhatten_dist())\n    jaccard_Similarity.append(sim_obj.Jaccard_Similarity())\n    stnd_ans.append(standard_answer)\n    stud_ans.append(student_answer)\n    _id.append(i)\n    actual_sentiments.append(dataframe['status'][i])\ndict_sample = {'Id':_id,'Standard_Answer': stnd_ans, 'Student_Answer': stud_ans,'Actual Sentiment':actual_sentiments,'Cosine_Similarity': cosine_similar,'Euclidian_Distance':euclid_dist,'Manhatten_Distance':manhatten_d,'JaccardSimilarity':jaccard_Similarity,} \n     \ndf = pd.DataFrame(dict_sample)\n   \ndf.to_csv('KeywordTest.csv')\n    ","metadata":{"id":"vs4SKCWcogeQ","execution":{"iopub.status.busy":"2022-01-11T18:28:08.225233Z","iopub.execute_input":"2022-01-11T18:28:08.225768Z","iopub.status.idle":"2022-01-11T18:28:10.253806Z","shell.execute_reply.started":"2022-01-11T18:28:08.225730Z","shell.execute_reply":"2022-01-11T18:28:10.253057Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 509.26it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 354.16it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa770dd84d1c4f2aa57eb2185b9b81c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed25b54db8a44cc083c9316ae2acdce8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 326.63it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 608.93it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7daca616615342cf824defbe18b06404"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9754124e6c444b8e5f8ad8fe78219a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 288.29it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 496.02it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e982b6dc9047fc83681c8b73a38593"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b9559bcff348c09512568b65568bb4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 440.81it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 642.02it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6307b1cfeb834741b740a4295052419d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeafdaf891954d4fa26bc6abdcca4db8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 236.27it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 503.03it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742e9b205e754bd0a51d5aed57169581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c998fe5043c42c7988f3ae7044f9d2f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 535.60it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 680.12it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac777dbd5424170a634cf244d24028e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33b080eeaf7b41609a401f6c825c78dc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 225.86it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 470.32it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a46ae58928d7441486e2a77a9d48c2c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f263f85040ff406a8033a4e4ee842c76"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 328.94it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 365.13it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff893905e4de4c16b09d4778a4de43cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff18d532ac548a7b562c1a004eeb271"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 270.25it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 447.54it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510b018764f04c9fb2329204a72153a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c3f0e17bfea48b79d1b1fab05f765e1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 298.15it/s]\n/opt/conda/lib/python3.7/site-packages/keybert/_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n  warnings.warn(\"Although extracting keywords for multiple documents is faster \"\n1it [00:00, 499.92it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70db90a1c654e0cb36480bb79d6c4b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958842f73f2840bf89867190a2377be3"}},"metadata":{}}]},{"cell_type":"code","source":"# print(sim_obj.cosine_similarity())\n# print(sim_obj.euclidian_dist())\n# print(sim_obj.manhatten_dist())\n# print(sim_obj.Jaccard_Similarity()) ","metadata":{"id":"KxBXGf30ojeo","outputId":"88e13e09-3fc8-45f3-9f93-c83d1b4f1b1f","execution":{"iopub.status.busy":"2022-01-11T18:20:48.341349Z","iopub.status.idle":"2022-01-11T18:20:48.342185Z","shell.execute_reply.started":"2022-01-11T18:20:48.341912Z","shell.execute_reply":"2022-01-11T18:20:48.341936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Jaccard_Similarity(standard_answer,student_answer)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T18:20:48.343468Z","iopub.status.idle":"2022-01-11T18:20:48.344280Z","shell.execute_reply.started":"2022-01-11T18:20:48.344041Z","shell.execute_reply":"2022-01-11T18:20:48.344067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def df_plot(result,stndrd_keyword=candidates_standard,student_keyword=candidates_student):\n#     return pd.DataFrame(result,columns=student_keyword,index=stndrd_keyword)","metadata":{"id":"-YK08fhlgacY","execution":{"iopub.status.busy":"2022-01-11T18:20:48.345417Z","iopub.status.idle":"2022-01-11T18:20:48.346214Z","shell.execute_reply.started":"2022-01-11T18:20:48.345982Z","shell.execute_reply":"2022-01-11T18:20:48.346005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_plot(sim_obj.cosine_similarity())","metadata":{"execution":{"iopub.status.busy":"2022-01-11T18:20:48.347321Z","iopub.status.idle":"2022-01-11T18:20:48.348138Z","shell.execute_reply.started":"2022-01-11T18:20:48.347905Z","shell.execute_reply":"2022-01-11T18:20:48.347928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_plot(sim_obj.jaccard_distance())","metadata":{"execution":{"iopub.status.busy":"2022-01-11T18:20:48.349245Z","iopub.status.idle":"2022-01-11T18:20:48.350080Z","shell.execute_reply.started":"2022-01-11T18:20:48.349846Z","shell.execute_reply":"2022-01-11T18:20:48.349870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_plot(sim_obj.euclidian_dist())","metadata":{"execution":{"iopub.status.busy":"2022-01-11T18:20:48.351193Z","iopub.status.idle":"2022-01-11T18:20:48.351996Z","shell.execute_reply.started":"2022-01-11T18:20:48.351762Z","shell.execute_reply":"2022-01-11T18:20:48.351786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_plot(sim_obj.manhatten_dist())","metadata":{"execution":{"iopub.status.busy":"2022-01-11T18:20:48.353102Z","iopub.status.idle":"2022-01-11T18:20:48.353907Z","shell.execute_reply.started":"2022-01-11T18:20:48.353672Z","shell.execute_reply":"2022-01-11T18:20:48.353696Z"},"trusted":true},"execution_count":null,"outputs":[]}]}