{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip \n! unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip","metadata":{"_uuid":"0d89fec7-5d44-4336-9da5-469346fa2def","_cell_guid":"b5e152ff-6613-4ecf-a299-d475fca64685","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:16:24.849677Z","iopub.execute_input":"2022-01-22T07:16:24.850102Z","iopub.status.idle":"2022-01-22T07:16:29.158720Z","shell.execute_reply.started":"2022-01-22T07:16:24.850043Z","shell.execute_reply":"2022-01-22T07:16:29.157940Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Archive:  ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n  inflating: train.csv               \nArchive:  ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n  inflating: test_labels.csv         \nArchive:  ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n  inflating: sample_submission.csv   \nArchive:  ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n  inflating: test.csv                \n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install transformers\n# filter the data, take the same portion of each sense\n# save the best weight ,means less loss valued trained batch \n# do some warm up step initially to make it more effiecient","metadata":{"_uuid":"a399aa52-641d-4b0b-ae94-428c6c4e20ff","_cell_guid":"76f4bbc5-95f6-4206-9caa-51d8fbd1b46b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:16:31.568689Z","iopub.execute_input":"2022-01-22T07:16:31.568962Z","iopub.status.idle":"2022-01-22T07:16:40.079729Z","shell.execute_reply.started":"2022-01-22T07:16:31.568931Z","shell.execute_reply":"2022-01-22T07:16:40.078919Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.8.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.2)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport transformers\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"262ea2b4-8965-457a-ac3f-36f3956394aa","_cell_guid":"f6f21706-c522-41f0-8eee-ec41acbad32e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:16:44.459290Z","iopub.execute_input":"2022-01-22T07:16:44.459652Z","iopub.status.idle":"2022-01-22T07:16:49.877429Z","shell.execute_reply.started":"2022-01-22T07:16:44.459604Z","shell.execute_reply":"2022-01-22T07:16:49.876639Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\") \nbert_model.trainable = False","metadata":{"_uuid":"6bdfa15e-a5d6-4c58-b132-4e686af50782","_cell_guid":"b49e404e-c101-479b-9c40-39d581a7c7e6","execution":{"iopub.status.busy":"2022-01-22T07:16:52.468625Z","iopub.execute_input":"2022-01-22T07:16:52.468901Z","iopub.status.idle":"2022-01-22T07:17:25.632775Z","shell.execute_reply.started":"2022-01-22T07:16:52.468870Z","shell.execute_reply":"2022-01-22T07:17:25.632123Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94813c7fbb74e9d8ae6292009416816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b6212d342447f49f49e7d15eeb9def"}},"metadata":{}},{"name":"stderr","text":"2022-01-22 07:17:18.973034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:18.974024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:18.974687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:18.975550: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-01-22 07:17:18.976464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:18.977147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:18.977774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:23.310301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:23.311121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:23.311765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-22 07:17:23.312339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size=32\nmax_len=128\nEPOCHS=2","metadata":{"_uuid":"0a70ea60-c055-442d-a17b-92aa31a13d59","_cell_guid":"57e0530c-53c6-4440-8074-c165b041a7c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:17:29.701473Z","iopub.execute_input":"2022-01-22T07:17:29.701735Z","iopub.status.idle":"2022-01-22T07:17:29.706300Z","shell.execute_reply.started":"2022-01-22T07:17:29.701707Z","shell.execute_reply":"2022-01-22T07:17:29.705070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"./train.csv\",nrows=75000)\ntest_labels=pd.read_csv(\"./test_labels.csv\",nrows=75000)\n# train_data.head() \n# print(data.loc[0])\n# data.columns\n\n# data[\"toxic\"].value_counts()\ndata.head()\ntest_labels.head()","metadata":{"_uuid":"9ee0f4d2-28db-46e3-84bb-f3881399e437","_cell_guid":"ec8e3e79-f162-49a5-b966-2b0edae9ab30","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:26:00.641666Z","iopub.execute_input":"2022-01-22T07:26:00.642466Z","iopub.status.idle":"2022-01-22T07:26:01.110854Z","shell.execute_reply.started":"2022-01-22T07:26:00.642425Z","shell.execute_reply":"2022-01-22T07:26:01.110036Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                 id  toxic  severe_toxic  obscene  threat  insult  \\\n0  00001cee341fdb12     -1            -1       -1      -1      -1   \n1  0000247867823ef7     -1            -1       -1      -1      -1   \n2  00013b17ad220c46     -1            -1       -1      -1      -1   \n3  00017563c3f7919a     -1            -1       -1      -1      -1   \n4  00017695ad8997eb     -1            -1       -1      -1      -1   \n\n   identity_hate  \n0             -1  \n1             -1  \n2             -1  \n3             -1  \n4             -1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"row_no=list(data.shape)[0]\ndata[\"comment_text\"]=data[\"comment_text\"].map(lambda x:re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x))","metadata":{"_uuid":"172c050f-530a-4cb9-bfeb-644d1d05e6bb","_cell_guid":"4e553d8c-573a-4b6a-a593-d18eed422cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:26:03.440942Z","iopub.execute_input":"2022-01-22T07:26:03.441398Z","iopub.status.idle":"2022-01-22T07:26:11.441763Z","shell.execute_reply.started":"2022-01-22T07:26:03.441358Z","shell.execute_reply":"2022-01-22T07:26:11.440927Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] \nsense_count_pd=pd.DataFrame(data[label_cols].value_counts()) \nsense_count_pd","metadata":{"_uuid":"acb671f4-2798-46d8-a568-1f0fd86dd624","_cell_guid":"f5302b70-2145-4634-87fa-67fff1e297de","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-01-22T07:26:11.443344Z","iopub.execute_input":"2022-01-22T07:26:11.443586Z","iopub.status.idle":"2022-01-22T07:26:11.470113Z","shell.execute_reply.started":"2022-01-22T07:26:11.443552Z","shell.execute_reply":"2022-01-22T07:26:11.469423Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                                                            0\ntoxic severe_toxic obscene threat insult identity_hate       \n0     0            0       0      0      0              67289\n1     0            0       0      0      0               2719\n                   1       0      1      0               1799\n                                  0      0                831\n                   0       0      1      0                598\n      1            1       0      1      0                473\n      0            1       0      1      1                296\n0     0            1       0      0      0                140\n                   0       0      1      0                133\n1     1            1       0      1      1                111\n0     0            1       0      1      0                 88\n1     1            1       0      0      0                 73\n      0            1       1      1      0                 68\n                   0       0      0      1                 58\n                                  1      1                 55\n                           1      0      0                 54\n      1            1       1      1      0                 38\n      0            1       1      1      1                 27\n0     0            0       0      0      1                 25\n1     1            0       0      0      0                 18\n                   1       1      1      1                 17\n      0            1       0      0      1                 15\n0     0            0       0      1      1                 10\n                           1      0      0                  9\n                   1       0      1      1                  9\n1     1            0       1      0      0                  8\n      0            0       1      1      0                  7\n                   1       1      0      0                  7\n      1            1       0      0      1                  4\n                   0       0      1      0                  4\n                                         1                  3\n      0            0       1      1      1                  3\n      1            0       0      0      1                  2\n0     0            0       1      1      0                  2\n1     1            1       1      0      0                  2\n      0            0       1      0      1                  2\n0     0            1       0      0      1                  1\n                           1      0      0                  1\n                                  1      0                  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>67289</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1</th>\n      <th rowspan=\"4\" valign=\"top\">0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>2719</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>1</th>\n      <th>0</th>\n      <td>1799</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <td>831</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <td>598</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <td>473</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <td>296</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th rowspan=\"7\" valign=\"top\">1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th rowspan=\"4\" valign=\"top\">0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th rowspan=\"8\" valign=\"top\">1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">0</th>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <th>0</th>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th rowspan=\"3\" valign=\"top\">0</th>\n      <th rowspan=\"3\" valign=\"top\">1</th>\n      <th>0</th>\n      <th>0</th>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">1</th>\n      <th>0</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"labels =  test_labels[label_cols].values","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:26:17.221603Z","iopub.execute_input":"2022-01-22T07:26:17.221862Z","iopub.status.idle":"2022-01-22T07:26:17.227390Z","shell.execute_reply.started":"2022-01-22T07:26:17.221833Z","shell.execute_reply":"2022-01-22T07:26:17.226655Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# indexes = np.arange(12)\n# np.array(labels[indexes], dtype=\"int32\")\nlabels","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:26:19.091520Z","iopub.execute_input":"2022-01-22T07:26:19.091789Z","iopub.status.idle":"2022-01-22T07:26:19.097399Z","shell.execute_reply.started":"2022-01-22T07:26:19.091759Z","shell.execute_reply":"2022-01-22T07:26:19.096592Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array([[-1, -1, -1, -1, -1, -1],\n       [-1, -1, -1, -1, -1, -1],\n       [-1, -1, -1, -1, -1, -1],\n       ...,\n       [-1, -1, -1, -1, -1, -1],\n       [-1, -1, -1, -1, -1, -1],\n       [ 0,  0,  0,  0,  0,  0]])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ninput_sen=data[\"comment_text\"].values\n# print(input_sen)\ntrain_inputs,validation_inputs,train_labels,validation_labels=train_test_split(input_sen,labels,random_state=0,test_size=0.2) \n\n\nprint(train_inputs.shape)\nprint(train_labels.shape)\n\nprint(validation_inputs.shape)\nprint(validation_labels.shape)\n ","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:26:21.450593Z","iopub.execute_input":"2022-01-22T07:26:21.451411Z","iopub.status.idle":"2022-01-22T07:26:21.471023Z","shell.execute_reply.started":"2022-01-22T07:26:21.451359Z","shell.execute_reply":"2022-01-22T07:26:21.470344Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"(60000,)\n(60000, 6)\n(15000,)\n(15000, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"class BertSemanticDataGenerator(tf.keras.utils.Sequence): \n    def __init__(\n        self,\n        sentence_pairs,\n        labels,\n        batch_size=batch_size,\n        shuffle=True,\n        include_targets=True,\n    ):\n        self.sentence_pairs = sentence_pairs\n        self.labels = labels\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.include_targets = include_targets\n         \n        \n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.indexes = np.arange(len(self.sentence_pairs))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Denotes the number of batches per epoch.\n        return len(self.sentence_pairs) // self.batch_size\n\n    def __getitem__(self, idx):\n        # Retrieves the batch of index.\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        sentence_pairs = self.sentence_pairs[indexes]\n\n        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n        # encoded together and separated by [SEP] token.\n        encoded = self.tokenizer.batch_encode_plus(\n            sentence_pairs.tolist(),\n            add_special_tokens=True,\n            max_length=128,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_tensors=\"tf\",\n        )   \n\n        bert_output = bert_model(**encoded)\n        \n        sequence_output = bert_output.last_hidden_state \n         \n        if self.include_targets:\n            labels = np.array(self.labels[indexes], dtype=\"int32\")\n            return sequence_output, labels\n        else:\n            return sequence_output\n\n    def on_epoch_end(self): \n        if self.shuffle:\n            np.random.RandomState(42).shuffle(self.indexes)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:26:24.144144Z","iopub.execute_input":"2022-01-22T07:26:24.145005Z","iopub.status.idle":"2022-01-22T07:26:24.155686Z","shell.execute_reply.started":"2022-01-22T07:26:24.144957Z","shell.execute_reply":"2022-01-22T07:26:24.154969Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataset=BertSemanticDataGenerator(train_inputs,train_labels,shuffle=True)\nvalidation_dataset=BertSemanticDataGenerator(validation_inputs,validation_labels,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:26:27.840581Z","iopub.execute_input":"2022-01-22T07:26:27.840833Z","iopub.status.idle":"2022-01-22T07:26:32.670757Z","shell.execute_reply.started":"2022-01-22T07:26:27.840805Z","shell.execute_reply":"2022-01-22T07:26:32.670061Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# for d in tqdm(train_dataset):\n#     print(d)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = tf.keras.layers.Input(shape=(128, 768), name=None)  \nflat=tf.keras.layers.Flatten()(input_layer) \noutput = tf.keras.layers.Dense(6, activation=\"softmax\")(flat)\nmodel = tf.keras.models.Model(inputs=input_layer, outputs=output)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:18:21.850049Z","iopub.execute_input":"2022-01-22T07:18:21.850781Z","iopub.status.idle":"2022-01-22T07:18:21.880807Z","shell.execute_reply.started":"2022-01-22T07:18:21.850741Z","shell.execute_reply":"2022-01-22T07:18:21.880125Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 128, 768)]        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 98304)             0         \n_________________________________________________________________\ndense (Dense)                (None, 6)                 589830    \n=================================================================\nTotal params: 589,830\nTrainable params: 589,830\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for step,(x_batch_train,labels) in enumerate(tqdm(train_dataset)):\n    print(labels.flatten().shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label","metadata":{"execution":{"iopub.status.busy":"2022-01-22T06:06:51.824179Z","iopub.execute_input":"2022-01-22T06:06:51.824458Z","iopub.status.idle":"2022-01-22T06:06:51.849327Z","shell.execute_reply.started":"2022-01-22T06:06:51.824424Z","shell.execute_reply":"2022-01-22T06:06:51.848496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# custom training loop \n## update at each train step\n## reset at the end of each batch\n\nimport time\n## defining a optimizer \noptimizer=tf.keras.optimizers.SGD(learning_rate=1e-3)\n\n## defining loss function \nloss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n# ## mean loss define\n# train_loss=tf.keras.metrics.Mean(name=\"train_loss\")\n# validation_loss=tf.keras.metrics.Mean(name=\"validation_loss\")\n\n# Metric\n## dfining the accuracy metric to track our model accuracy.Here for 6 class we \n## have to declare 2d darray of row 6\ntrain_acc_metric=[tf.keras.metrics.CategoricalAccuracy() for i in range(len(label_cols))]\n\nval_acc_metric=[tf.keras.metrics.CategoricalAccuracy() for i in range(len(label_cols))]\n\n# actually from logits denoting the probability from our custom model layed for each label.It is being fetched before the softmax layer to calculate loss between actual and predicted\n\nbatch_size=32\nEPOCH=2\ntrain_dataset_size=60000\nvalidation_dataset_size=15000\n\n@tf.function\ndef train_step(model,x_train,label):\n    # Gradiane tape actually records the operation run in forward step\n    with tf.GradientTape() as tape:\n        #caluculate logits for comparison\n        logits_prob=model(x_train,training=True)\n        # calculate loss value \n        loss_value=loss_fn(label,logits_prob)\n    #calculate gradient of trainable variables against the loss\n    gradients=tape.gradient(loss_value,model.trainable_weights)\n    # update the gradient according to gradient descent\n    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n    # update the mean train ing loss\n    train_loss(loss_value)\n    # update accuracy metric for each of the 6 classes \n    for i,auc in enumerate(train_acc_metric):\n        auc.update_state(label[:,i],logits_prob[:,i])\n    return loss_value\n\n@tf.function\ndef validation_step(model,x_validation,label):\n    with tf.GradientTape() as tape:\n        validation_logit_prob=model(x_validation,training=False)\n        valid_loss=loss_fn(label,validation_logit_prob)\n        validation_loss(valid_loss)\n        for i,auc in enumerate(val_acc_metric):\n            auc.update_state(label[:,i],validation_logit_prob[:,i]) \n\ndef train_model(model,train_dataset,validation_dataset):\n    for epoch in range(EPOCHS):\n        print('\\n Epoch No %d\\n' % (epoch,))\n\n        ### training part ###\n        for step,(x_batch_train,labels) in enumerate(tqdm(train_dataset)):\n            training_loss=train_step(model,x_batch_train,labels)\n            \n            #log result at every 200 batches\n            if step%20==0:\n                print(f'\\nTrain Step: {epoch}, Loss: {train_loss.result()}')\n#                 print(\"Trainng loss at %d batch of data: %.4f\"%(step,float(training_loss)))\n                # training accuracy metric at end\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_acc_metric[i].result()}\")\n                    # reset the accuracy metric after every epoch\n                    train_acc_metric[i].reset_states()\n            \n#         training_accuracy=train_acc_metric.result()\n#         print(\"\\nTraining accuracy after %d epoch : %.4f\"%(epoch,training_accuracy))\n#         train_acc_metric.reset_states()\n        \n        \n        ### validation part ###\n        for step,(x_batch_val,labels) in enumerate(tqdm(validation_dataset)):\n            validation_step(model,x_batch_val,labels)\n        print(f'\\nTrain Step: {epoch}, Loss: {validation_loss.result()}')\n        \n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {val_acc_metric[i].result()}\") \n            val_acc_metric[i].reset_states()\n#         validation_acc=val_acc_metric.result()\n#         val_acc_metric.reset_states()\n#         print(\"\\n validation accuracy : %4.f\"%(validation_acc))\n\ntrain_model(model,train_dataset,validation_dataset)\nmodel.save(\"my_custom_train_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:27:08.032123Z","iopub.execute_input":"2022-01-22T07:27:08.032555Z","iopub.status.idle":"2022-01-22T07:44:56.040321Z","shell.execute_reply.started":"2022-01-22T07:27:08.032517Z","shell.execute_reply":"2022-01-22T07:44:56.039602Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"\n Epoch No 0\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1875 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed89912d48e64fe8962019e0c5f4cf73"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/keras/backend.py:4847: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n  '\"`categorical_crossentropy` received `from_logits=True`, but '\n","output_type":"stream"},{"name":"stdout","text":"\nTrain Step: 0, Loss: -6.22709321975708\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.277719974517822\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.255007266998291\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.274140357971191\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.20000000298023224\n\nTrain Step: 0, Loss: -6.31077766418457\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.330654621124268\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.307281494140625\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.318582534790039\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.351726055145264\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.339192867279053\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.3342084884643555\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.344011306762695\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.345230579376221\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.359987258911133\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.378695487976074\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.379525661468506\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.373666763305664\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.363812446594238\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.349287509918213\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.372028350830078\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.368658065795898\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.378057956695557\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.363804817199707\ntoxic roc_auc 0.20000000298023224\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.360288143157959\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.364381790161133\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.371230602264404\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.376990795135498\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.376348972320557\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.367179870605469\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.369196891784668\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.362687587738037\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.362916946411133\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.354252338409424\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.354084491729736\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.345978260040283\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.3504533767700195\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.349440097808838\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.360561847686768\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.366114616394043\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.366003036499023\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.367507457733154\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.372392177581787\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.375674724578857\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.372000217437744\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.36745023727417\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.372407913208008\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.37184476852417\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.374027729034424\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.371284484863281\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.3654093742370605\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.36939001083374\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.15000000596046448\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.372483730316162\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.373253345489502\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.3750224113464355\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.369850158691406\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.370854377746582\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.363493919372559\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.3661065101623535\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.3610920906066895\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.373358726501465\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.376580715179443\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.382363796234131\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.25\n\nTrain Step: 0, Loss: -6.374403476715088\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.3730645179748535\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.379000663757324\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.3789262771606445\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.375791549682617\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.374846458435059\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.372494697570801\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.372315406799316\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.369021892547607\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.364799499511719\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.364939212799072\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.15000000596046448\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.20000000298023224\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.3636298179626465\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.362843990325928\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.360872268676758\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.356205463409424\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.357840061187744\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.352652072906494\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.35101318359375\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.347713470458984\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.348060607910156\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.351597785949707\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.351582050323486\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.349495887756348\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 0, Loss: -6.352741241455078\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.354015827178955\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.354093551635742\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.352567672729492\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.354456901550293\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.355422019958496\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.3530097007751465\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 0, Loss: -6.352884769439697\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 0, Loss: -6.351700782775879\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a25b09cd5a84d8eb057b100c29b28ef"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"\nTrain Step: 0, Loss: -6.311530113220215\ntoxic roc_auc 0.04273504391312599\nsevere_toxic roc_auc 0.03846153989434242\nobscene roc_auc 0.025641025975346565\nthreat roc_auc 0.03205128386616707\ninsult roc_auc 0.03418803587555885\nidentity_hate roc_auc 0.03846153989434242\n\n Epoch No 1\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1875 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e744bbddf934471890224e36b00a864"}},"metadata":{}},{"name":"stdout","text":"\nTrain Step: 1, Loss: -6.3551836013793945\ntoxic roc_auc 0.06666667014360428\nsevere_toxic roc_auc 0.13333334028720856\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.3550825119018555\ntoxic roc_auc 0.25\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.351948261260986\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.351147651672363\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.15000000596046448\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.20000000298023224\n\nTrain Step: 1, Loss: -6.352055549621582\ntoxic roc_auc 0.20000000298023224\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.352416515350342\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.349170207977295\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.349063873291016\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.351484775543213\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.349238395690918\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.347601890563965\ntoxic roc_auc 0.20000000298023224\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.347957611083984\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.347215175628662\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.348522186279297\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.350797176361084\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.350409507751465\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.348963737487793\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.346697807312012\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.343520641326904\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.347193717956543\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.346088886260986\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.347580909729004\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.34425687789917\ntoxic roc_auc 0.25\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.342951774597168\ntoxic roc_auc 0.20000000298023224\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.34345006942749\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.34453010559082\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.345565319061279\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.345144748687744\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.342443466186523\ntoxic roc_auc 0.20000000298023224\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.342684745788574\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.340651035308838\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.340446949005127\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.337771415710449\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.337387561798096\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.334761142730713\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.335601329803467\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.334928512573242\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.337872505187988\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.339310646057129\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.3390045166015625\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.339223384857178\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.34053373336792\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.341421604156494\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.340063571929932\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.338409900665283\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.339874744415283\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.3394455909729\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.340034484863281\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.338937759399414\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.336672782897949\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.337970733642578\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.338931560516357\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.33909797668457\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.3396196365356445\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.337586879730225\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.337795734405518\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.334908962249756\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.335781097412109\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.333719253540039\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.338413238525391\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.15000000596046448\n\nTrain Step: 1, Loss: -6.339648246765137\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.341846942901611\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.15000000596046448\n\nTrain Step: 1, Loss: -6.3385701179504395\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.337939262390137\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.34033727645874\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.340269565582275\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.338845729827881\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.338376998901367\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.337339401245117\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.337173938751221\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.3357014656066895\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.333809852600098\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.333803176879883\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.15000000596046448\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.333118438720703\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.10000000149011612\n\nTrain Step: 1, Loss: -6.332695484161377\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.331694602966309\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.329497337341309\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.15000000596046448\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.33017635345459\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.15000000596046448\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.327736854553223\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.326858997344971\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.325230121612549\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.325277805328369\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.326810836791992\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.326731204986572\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.10000000149011612\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.32563591003418\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.05000000074505806\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.327105522155762\ntoxic roc_auc 0.15000000596046448\nsevere_toxic roc_auc 0.10000000149011612\nobscene roc_auc 0.0\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.327653408050537\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.20000000298023224\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.327602386474609\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.3267741203308105\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.3275909423828125\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.0\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.0\n\nTrain Step: 1, Loss: -6.328025817871094\ntoxic roc_auc 0.10000000149011612\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.10000000149011612\ninsult roc_auc 0.0\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.326786041259766\ntoxic roc_auc 0.0\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.10000000149011612\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.326614856719971\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.05000000074505806\nthreat roc_auc 0.05000000074505806\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n\nTrain Step: 1, Loss: -6.325972080230713\ntoxic roc_auc 0.05000000074505806\nsevere_toxic roc_auc 0.0\nobscene roc_auc 0.0\nthreat roc_auc 0.0\ninsult roc_auc 0.05000000074505806\nidentity_hate roc_auc 0.05000000074505806\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5f8af2202947f995c2ca558efa30b7"}},"metadata":{}},{"name":"stdout","text":"\nTrain Step: 1, Loss: -6.292143821716309\ntoxic roc_auc 0.03846153989434242\nsevere_toxic roc_auc 0.03846153989434242\nobscene roc_auc 0.025641025975346565\nthreat roc_auc 0.03418803587555885\ninsult roc_auc 0.03846153989434242\nidentity_hate roc_auc 0.044871795922517776\n","output_type":"stream"}]},{"cell_type":"code","source":"# from keras.models import load_model\n# model=load_model('../input/mymodel/my_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s4 = 'fuck you bitch!!!'\nsentence_pairs = np.array([s4])\ntest_data = BertSemanticDataGenerator(\n        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n    )\n\npro=model.predict(test_data)\nprint(pro)\nprint(label_cols[np.argmax(pro)])","metadata":{"execution":{"iopub.status.busy":"2022-01-22T07:49:28.184286Z","iopub.execute_input":"2022-01-22T07:49:28.184551Z","iopub.status.idle":"2022-01-22T07:49:30.828111Z","shell.execute_reply.started":"2022-01-22T07:49:28.184514Z","shell.execute_reply":"2022-01-22T07:49:30.827296Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"[[0.24514431 0.18649596 0.16171435 0.14327097 0.12693104 0.13644336]]\ntoxic\n","output_type":"stream"}]},{"cell_type":"code","source":"# loading the pre-defined bert model weights\nbert_model.Trainable=True\n\ntrain_dataset=BertSemanticDataGenerator(train_inputs,train_labels,shuffle=True)\nvalidation_dataset=BertSemanticDataGenerator(validation_inputs,validation_labels,shuffle=False)\n\ntrained_history = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.trainable_variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df=pd.read_csv(\"./sample_submission.csv\",index_col='id')\n# test_df=pd.read_csv(\"./test.csv\")\n# label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# print(submission_df.head())\n# print(test_df.head())\n\n\n# test_bert_op=BertSemanticDataGenerator(test_df['comment_text'],None,include_targets=False,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i,sen in enumerate(tqdm(test_bert_op)):\n#     sample_ids = test_df.iloc[i*32:(i+1)*32]['id'] \n#     pred=model.predict(sen)\n#     submission_df.loc[sample_ids, label_cols] = pred\n#     print(pred)\n#     print(sen.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_ids = test_df.iloc[0*32:(0+1)*32]['id'] \n# print(sample_ids)\n# submission_df.loc[sample_ids]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"my_model.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.preprocessing.sequence import pad_sequences\n# # to convert the iput ids array in same size(column no=max(column no))\n# tokenizer=transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n# max_length=128\n# bert_model=transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n# bert_model.trainable=False\n\n# def tokenize(data,tokenizer=tokenizer,max_length=max_length):\n# #     input_ids=[]\n# #     attention_masks=[]\n#     bert_outputs=[]\n#     for sentence in tqdm(data):\n        \n#         encoded_data=tokenizer.batch_encode_plus(\n#                         sentence,\n#                         add_special_tokens=True,\n#                         max_length=max_length,\n#                         truncation=True,\n#                         return_attention_mask=True,\n#                         return_token_type_ids=True,\n#                         pad_to_max_length=True,\n#                         return_tensors=\"tf\",\n#                     )\n\n# #         input_id=np.array(encoded_data[\"input_ids\"],dtype=\"int32\")\n# #         attention_mask=np.array(encoded_data[\"attention_mask\"],dtype=\"int32\") \n        \n#         bert_output=bert_model(**encoded_data)\n#         sequence_output = bert_output.last_hidden_state\n#         bert_outputs.append(sequence_output)\n#     return bert_outputs\n# #         input_ids.append(input_id)\n# #         attention_masks.append(attention_mask)\n        \n# #     return [input_ids,attention_masks]\n\n# bert_op=tokenize(data['comment_text'])\n# # input_ids=pad_sequences(bert_op[0],maxlen=max_length,dtype='long',value=0,truncating=\"post\",padding=\"post\")\n# # attention_masks=bert_op[1]\n# # bert_op.shape","metadata":{"_uuid":"b3c9a14a-97f5-4524-8ee3-1f26b7cd3bc3","_cell_guid":"89d44245-7af1-442a-bdd4-4a9c3e2185f3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(bert_op)","metadata":{"_uuid":"8959229a-8e2d-4b3b-aac2-0fb6ce9a033d","_cell_guid":"54912915-4e18-4d75-91e8-026f9ce2e866","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attention_masks=np.array(attention_masks)\n# attention_masks=pad_sequences(attention_masks,maxlen=max_length,dtype='long',value=0,truncating=\"post\",padding=\"post\")\n# input_ids.shape\n# attention_masks.shape","metadata":{"_uuid":"d3dc7179-5583-4c1a-b5ba-8e66a821990e","_cell_guid":"c50fc5aa-5eb4-4d52-975d-b04430e24c0b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # creating batched dataset\n# epochs=2\n# def create_batch_dataset(data,epochs=epochs,batch_size=batch_size,buffer_size=1000,train=True):\n#     dataset=tf.data.Dataset.from_tensor_slices(data)\n# #     print(dataset.as_numpy_iterator())\n#     if train:\n#         dataset=dataset.shuffle(buffer_size=buffer_size)\n#         # uses for shuffling the dataset.Select the first buffer_size element from dataset\n#     dataset=dataset.repeat(epochs)\n#     # just repeat the whole dataset\n#     dataset=dataset.batch(batch_size=batch_size)\n#     # devide the whole dataset into batch size and create an array of array\n#     if train:\n#         dataset=dataset.prefetch(1)\n#     #     It has no concept of examples vs. batches. examples.prefetch(2) will prefetch two \n#     # elements (2 examples), while examples.batch(20).prefetch(2) will prefetch 2 elements (2 \n#     # batches, of 20 examples each).\n#     return dataset\n# train_dataset=create_batch_dataset((train_inputs,train_masks,train_labels),train=True)\n# validation_dataset=create_batch_dataset((validation_inputs,validation_masks,validation_labels),train=True)","metadata":{"_uuid":"d627f4cb-eafd-4f52-99d1-cacead47c1ca","_cell_guid":"0af7aedf-d747-4590-bec8-37a35f99eadb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_dataset.as_numpy_iterator())\n# print(validation_dataset.as_numpy_iterator())","metadata":{"_uuid":"73f7786f-9ca0-4f29-a82d-650477b7d2bc","_cell_guid":"a59210ea-f3ff-40dc-adc3-43704a4db6b2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list(train_dataset.as_numpy_iterator())[0][0].shape","metadata":{"_uuid":"427fde78-4770-4549-ae20-645d3f36761f","_cell_guid":"78a49bf9-1666-4840-905a-bca000fd79c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):\n#             print(token_ids.shape)","metadata":{"_uuid":"81665e1f-ce0f-4bff-9771-2257c2ee4ede","_cell_guid":"ea775813-0198-4389-9854-204d13b6e189","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/nkaenzig/bert-tensorflow-2-huggingface-transformers\n# https://www.kaggle.com/satyamkryadav/bert-model-96-77/notebook","metadata":{"_uuid":"cf97dec3-abcc-41fb-92d7-bfa598eacba0","_cell_guid":"0d34a9f6-e683-4261-b26e-d5e31d2ef1e6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}